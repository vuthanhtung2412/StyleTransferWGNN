{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTuLKDVrhwh",
        "outputId": "fb398bec-48fa-45a9-f733-2157c50dff4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8JtcgR6rPsw",
        "outputId": "80d275e3-10f7-49e2-dbc5-61ae6b287981"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available())\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8QOSFMfrPsz",
        "outputId": "b1f2f8a4-1d18-4038-daf6-33e8193eb791"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Create two matrices\u001b[39;00m\n\u001b[0;32m      4\u001b[0m A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create two matrices\n",
        "A = torch.rand(2,1, 4)\n",
        "B = torch.rand(2,4, 1)\n",
        "\n",
        "# Multiply the matrices\n",
        "C = torch.matmul(A, B)\n",
        "print(A)\n",
        "print(B)\n",
        "print(C)\n",
        "\n",
        "# Print the result\n",
        "print(C.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGFE5FFsrPs0"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9Gs9UFsrPs1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSjvGMstrPs2"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1p6OoYRrPs2",
        "outputId": "6d6e83a9-198e-457a-8d76-3d2b58566192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (33): ReLU(inplace=True)\n",
            "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (35): ReLU(inplace=True)\n",
            "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n",
            "(256, 128, 3, 3)\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "print(model.features)\n",
        "\n",
        "layer = model.features[10]\n",
        "\n",
        "weights = layer.weight.detach().cpu().numpy()\n",
        "\n",
        "print(weights.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTkCKIhrPs3"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvhMI0ZYrPs3",
        "outputId": "43ff4441-58b0-412c-f300-c72917183eaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[6, 3, 2],\n",
              "         [6, 3, 2],\n",
              "         [2, 3, 6],\n",
              "         [6, 2, 3]],\n",
              "\n",
              "        [[5, 1, 0],\n",
              "         [2, 0, 1],\n",
              "         [5, 1, 2],\n",
              "         [5, 4, 1]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def knn_graph(x, y, k):\n",
        "    '''\n",
        "    for each node in y, search for k-nearest-neighbor in x\n",
        "    :param x: BxNxE tensor, B batches, N nodes, E embeddings\n",
        "    :param y: BxMxE tensor, B batches, M nodes, E embeddings\n",
        "    :param k: k nearest neighbors\n",
        "    :return idx_k: BxMxK tensor, B batches, M node indices in y, K neighboring node indices in x\n",
        "    '''\n",
        "\n",
        "    D = normalized_cross_correlation(y, x.permute(0,2,1))\n",
        "\n",
        "    # print(D)\n",
        "    # print(D.shape)\n",
        "\n",
        "    _, idx_k = torch.topk(D, k, dim=2, largest=True, sorted=True)\n",
        "    # print(idx_k)\n",
        "    # print(idx_k.shape)\n",
        "    return idx_k\n",
        "\n",
        "def normalized_cross_correlation(x, y, eps=1e-8):\n",
        "    dev_xy = torch.matmul(x,y)\n",
        "    dev_xx = torch.mul(x,x)\n",
        "    dev_yy = torch.mul(y,y)\n",
        "\n",
        "    dev_xx_sum = torch.sum(dev_xx, dim=-1, keepdim=True)\n",
        "    dev_yy_sum = torch.sum(dev_yy, dim=-2, keepdim=True)\n",
        "\n",
        "    ncc = torch.div(dev_xy + eps, torch.sqrt( torch.mul(dev_xx_sum, dev_yy_sum)) + eps)\n",
        "    return ncc\n",
        "\n",
        "knn_graph(torch.rand(2,7,10),torch.rand(2,4, 10),3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjnuYugRrPs4"
      },
      "source": [
        "# GAT layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9b2cz-WrPs4",
        "outputId": "83fe74df-942c-4b16-bac4-b2a1c7580419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "s to c\n",
            "tensor([[[1, 2, 5],\n",
            "         [2, 4, 7],\n",
            "         [1, 3, 7],\n",
            "         [1, 5, 3],\n",
            "         [4, 2, 5],\n",
            "         [4, 7, 1],\n",
            "         [4, 1, 5],\n",
            "         [5, 4, 2]],\n",
            "\n",
            "        [[2, 0, 4],\n",
            "         [7, 6, 4],\n",
            "         [4, 2, 0],\n",
            "         [1, 4, 6],\n",
            "         [4, 3, 0],\n",
            "         [3, 7, 2],\n",
            "         [1, 4, 5],\n",
            "         [3, 7, 0]]])\n",
            "c to c\n",
            "tensor([[[0, 3, 1],\n",
            "         [1, 4, 0],\n",
            "         [2, 0, 5],\n",
            "         [3, 0, 4],\n",
            "         [4, 7, 1],\n",
            "         [5, 2, 3],\n",
            "         [6, 4, 7],\n",
            "         [7, 4, 6]],\n",
            "\n",
            "        [[0, 2, 4],\n",
            "         [1, 5, 3],\n",
            "         [2, 0, 5],\n",
            "         [3, 1, 7],\n",
            "         [4, 0, 6],\n",
            "         [5, 1, 2],\n",
            "         [6, 3, 2],\n",
            "         [7, 3, 5]]])\n",
            "--------------------\n",
            "tensor([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7,\n",
            "        0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7])\n",
            "tensor([[ 0,  3,  1,  1,  4,  0,  2,  0,  5,  3,  0,  4,  4,  7,  1,  5,  2,  3,\n",
            "          6,  4,  7,  7,  4,  6,  9, 10, 13, 10, 12, 15,  9, 11, 15,  9, 13, 11,\n",
            "         12, 10, 13, 12, 15,  9, 12,  9, 13, 13, 12, 10],\n",
            "        [ 0,  2,  4,  1,  5,  3,  2,  0,  5,  3,  1,  7,  4,  0,  6,  5,  1,  2,\n",
            "          6,  3,  2,  7,  3,  5, 10,  8, 12, 15, 14, 12, 12, 10,  8,  9, 12, 14,\n",
            "         12, 11,  8, 11, 15, 10,  9, 12, 13, 11, 15,  8]])\n",
            "cpu\n",
            "tensor([[[-0.0914,  0.0862, -0.0526,  0.1710, -0.0866,  0.2784, -0.2089,\n",
            "           0.0731, -0.1048],\n",
            "         [-0.0807,  0.0779, -0.0700,  0.1680, -0.0646,  0.2906, -0.1921,\n",
            "           0.0640, -0.0919],\n",
            "         [-0.1186,  0.1109, -0.0558,  0.2131, -0.1186,  0.3379, -0.2671,\n",
            "           0.0952, -0.1363],\n",
            "         [-0.0755,  0.0727, -0.0636,  0.1557, -0.0613,  0.2682, -0.1790,\n",
            "           0.0599, -0.0859],\n",
            "         [-0.0622,  0.0626, -0.0879,  0.1537, -0.0327,  0.2882, -0.1588,\n",
            "           0.0484, -0.0698],\n",
            "         [-0.1117,  0.1048, -0.0583,  0.2048, -0.1088,  0.3291, -0.2534,\n",
            "           0.0895, -0.1282],\n",
            "         [-0.0505,  0.0531, -0.1007,  0.1458, -0.0118,  0.2895, -0.1384,\n",
            "           0.0386, -0.0559],\n",
            "         [-0.0505,  0.0531, -0.1006,  0.1457, -0.0118,  0.2893, -0.1383,\n",
            "           0.0385, -0.0558]],\n",
            "\n",
            "        [[-0.1257,  0.1185, -0.0727,  0.2356, -0.1189,  0.3838, -0.2874,\n",
            "           0.1006, -0.1441],\n",
            "         [-0.1240,  0.1152, -0.0497,  0.2165, -0.1284,  0.3368, -0.2764,\n",
            "           0.0998, -0.1427],\n",
            "         [-0.1358,  0.1263, -0.0551,  0.2377, -0.1403,  0.3703, -0.3031,\n",
            "           0.1093, -0.1563],\n",
            "         [-0.1190,  0.1098, -0.0371,  0.2003, -0.1285,  0.3032, -0.2619,\n",
            "           0.0960, -0.1373],\n",
            "         [-0.1086,  0.1046, -0.0910,  0.2236, -0.0885,  0.3848, -0.2573,\n",
            "           0.0861, -0.1237],\n",
            "         [-0.1383,  0.1277, -0.0457,  0.2345, -0.1480,  0.3572, -0.3051,\n",
            "           0.1115, -0.1594],\n",
            "         [-0.0991,  0.0956, -0.0859,  0.2061, -0.0793,  0.3566, -0.2357,\n",
            "           0.0785, -0.1127],\n",
            "         [-0.1337,  0.1227, -0.0341,  0.2195, -0.1482,  0.3263, -0.2918,\n",
            "           0.1080, -0.1544]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 8, 9])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  assert input.numel() == input.storage().size(), (\n"
          ]
        }
      ],
      "source": [
        "import dgl\n",
        "import dgl.nn.pytorch as dglnn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HeterConv(nn.Module):\n",
        "    def __init__(self, feat_c, patch_size):\n",
        "        super().__init__()\n",
        "        self.feat_c = feat_c\n",
        "        self.patch_size = patch_size\n",
        "        self.feat_pc = feat_c*patch_size*patch_size\n",
        "\n",
        "        # GATv2\n",
        "        self.conv1 = dglnn.GATv2Conv(self.feat_pc, self.feat_pc, num_heads=1, allow_zero_in_degree=True)\n",
        "        self.conv2 = dglnn.GATv2Conv(self.feat_pc, self.feat_pc, num_heads=1, allow_zero_in_degree=True)\n",
        "\n",
        "        # SwinIR\n",
        "        # self.conv1 = dglnn.TWIRLSConv(self.feat_pc, self.feat_pc, hidden_d = 64, prop_step = 2)\n",
        "        # self.conv2 = dglnn.TWIRLSConv(self.feat_pc, self.feat_pc, hidden_d = 64, prop_step = 2)\n",
        "\n",
        "    def forward(self, feat_c, feat_s, idx_k1, idx_k2):\n",
        "        b,n,f = feat_s.shape\n",
        "        # print(feat_s.shape) torch.Size([8, 784, 12800]) 12800=512*5*5 784=28*28\n",
        "        b,m,f = feat_c.shape\n",
        "        _,_,k = idx_k1.shape\n",
        "        c = self.feat_c; p = self.patch_size\n",
        "        dev=feat_c.device\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # indices\n",
        "        idx_v = torch.tensor(range(m), device=dev, dtype=torch.int64).view(1,m,1).expand(2,m,k).contiguous().view(-1)\n",
        "        print(idx_v)\n",
        "        # feat of content patch og\n",
        "        idx_u = torch.cat((idx_k2, idx_k1+m), dim=1).contiguous().view(b,-1)\n",
        "        print(idx_u)\n",
        "\n",
        "\n",
        "        # features\n",
        "        feat = torch.cat((feat_c, feat_s), dim=1)\n",
        "\n",
        "        agg = torch.tensor([]).to(dev)\n",
        "        for i in range(b):\n",
        "            # create graph\n",
        "            graph = dgl.graph((idx_u[i], idx_v), num_nodes=m+n)\n",
        "            if i == 0:\n",
        "                print(graph.device)\n",
        "            feat_out = self.conv(graph, feat[i])[:m] # m,f\n",
        "            agg = torch.cat((agg, feat_out.contiguous().view(1,m,f)), dim=0)\n",
        "\n",
        "        return agg\n",
        "\n",
        "    def conv(self, graph, x):\n",
        "        h = self.conv1(graph, x)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(graph, h)\n",
        "        return h\n",
        "\n",
        "h = HeterConv(1,3)\n",
        "print(next(h.conv1.parameters()).device)\n",
        "# batch, nb patch, nb feat\n",
        "feat_c = torch.rand(2,8,9)\n",
        "feat_s = torch.rand(2,8,9)\n",
        "idx_k1 = knn_graph(feat_s, feat_c, 3)\n",
        "idx_k2 = knn_graph(feat_c, feat_c, 3)\n",
        "print(\"s to c\")\n",
        "print(idx_k1)\n",
        "print(\"c to c\")\n",
        "print(idx_k2)\n",
        "print(\"--------------------\")\n",
        "out = h(feat_c, feat_s, idx_k1, idx_k2)\n",
        "print(out)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmcjEhTDrPs6"
      },
      "source": [
        "# ADAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwB-Pzq5rPs6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "\n",
        "def adaptive_instance_normalization(content_feat, style_feat):\n",
        "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc6khPaZrPs7"
      },
      "source": [
        "# extra methodes for patch2feat and feat2 patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzkLluCZrPs7"
      },
      "outputs": [],
      "source": [
        "def calc_padding(x_shape, patchsize, stride, padding=None):\n",
        "    if padding is None:\n",
        "        xdim = x_shape\n",
        "        padvert = -(xdim[0] - patchsize) % stride\n",
        "        padhorz = -(xdim[1] - patchsize) % stride\n",
        "        padtop = int(np.floor(padvert / 2.0))\n",
        "        padbottom = int(np.ceil(padvert / 2.0))\n",
        "        padleft = int(np.floor(padhorz / 2.0))\n",
        "        padright = int(np.ceil(padhorz / 2.0))\n",
        "    else:\n",
        "        padtop = padbottom = padleft = padright = padding\n",
        "    return padtop, padbottom, padleft, padright\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4McXGhlBrPs7"
      },
      "source": [
        "# patch2feat (non functional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8xsR5pcrPs8",
        "outputId": "c58042a4-7728-4ad8-c6b4-ece9664994b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900\n",
            "torch.Size([900, 243])\n",
            "torch.Size([2, 900, 243])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def extract_patches(images, patch_size, stride = 1):\n",
        "    patches = []\n",
        "    _, c, h, w = images.shape\n",
        "    print(((h - patch_size + 1)//stride)**2)\n",
        "\n",
        "    for image in images:\n",
        "        tmp = []\n",
        "\n",
        "        for y in range(0, h - patch_size + 1, stride):\n",
        "            for x in range(0, w - patch_size + 1, stride):\n",
        "                patch = image[:, y:y+patch_size, x:x+patch_size].reshape(-1)\n",
        "                tmp.append(patch)\n",
        "        patches.append(torch.stack(tmp))\n",
        "\n",
        "    print(patches[0].shape)\n",
        "    return torch.stack(patches)\n",
        "\n",
        "image_batch = torch.randn(2, 3, 128, 128)\n",
        "patch_size = 9\n",
        "stride = 4\n",
        "\n",
        "extracted_patches = extract_patches(image_batch, patch_size, stride)\n",
        "\n",
        "print(extracted_patches.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CufWCY3rPs8"
      },
      "source": [
        "# Functional Patch2feat with torch unfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7OCElY5rPs9",
        "outputId": "a10c2fdd-e6ab-44ae-a3b7-734c63fd9723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3844, 75])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "def extract_patches(images, patch_size = 7, stride = 2):\n",
        "    unfold = nn.Unfold(kernel_size=(patch_size, patch_size), stride = stride)\n",
        "    res = unfold(images)\n",
        "    b, e, n = res.shape\n",
        "    return res.view(b,n,e)\n",
        "\n",
        "image_batch = torch.randn(2, 3, 128, 128)\n",
        "patch_size = 5\n",
        "\n",
        "stride = 2\n",
        "\n",
        "extracted_patches = extract_patches(image_batch, patch_size, stride)\n",
        "print(extracted_patches.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyramid feature"
      ],
      "metadata": {
        "id": "-kGQiXlrMfhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "def pyramid_feature(image): # we use a 4 by 4 grid\n",
        "  model = models.vgg19(pretrained=True)\n",
        "  enc_1 = model.features[:18]  # relu3_1\n",
        "  enc_2 = model.features[18:27]  #relu4_1\n",
        "  feat_1 = enc_1(image)\n",
        "  feat_2 = enc_2(feat_1)\n",
        "  feat_1 = extract_patches(feat_1, 14, 2)\n",
        "  feat_2 = extract_patches(feat_2, 7, 1)\n",
        "  return torch.cat([feat_1,feat_2], dim = 2)\n",
        "\n",
        "print(pyramid_feature(torch.randn(2, 3, 224, 224)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywK9I3M0Mjn_",
        "outputId": "27662e18-d9ef-473b-b332-a2d0fa231007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 484, 75264])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLsRuKMqrPs9"
      },
      "source": [
        "# feat2patch (non functnional)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def reconstruct_image(patches, image_size, patch_size, patch_stride):\n",
        "    reconstructed = torch.zeros((1, 1, image_size, image_size))  # Initialize the reconstructed image tensor\n",
        "\n",
        "    patch_count = 0\n",
        "    for y in range(0, image_size - patch_size + 1, patch_stride):\n",
        "        for x in range(0, image_size - patch_size + 1, patch_stride):\n",
        "            if patch_count < len(patches):  # Check if the patch count is within the valid range\n",
        "                patch = patches[patch_count]\n",
        "                reconstructed[:, :, y:y+patch_size, x:x+patch_size] += patch\n",
        "                patch_count += 1\n",
        "\n",
        "    return reconstructed / (patch_count // (patch_stride ** 2))  # Normalize the reconstructed image\n",
        "\n",
        "# Example usage\n",
        "image = torch.tensor([[1, 2, 3, 4, 5],\n",
        "                      [6, 7, 8, 9, 10],\n",
        "                      [11, 12, 13, 14, 15],\n",
        "                      [16, 17, 18, 19, 20],\n",
        "                      [21, 22, 23, 24, 25]], dtype=torch.float32)\n",
        "\n",
        "patch_size = 3\n",
        "patch_stride = 2\n",
        "\n",
        "patches = extract_patches(image, patch_size, patch_stride)\n",
        "\n",
        "image_size = len(image)\n",
        "reconstructed_image = reconstruct_image(patches, image_size, patch_size, patch_stride)\n",
        "\n",
        "print(reconstructed_image.squeeze(0).squeeze(0))  # Print the reconstructed image\n"
      ],
      "metadata": {
        "id": "OPrkxBh0va88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functional feat2patch with torch fold"
      ],
      "metadata": {
        "id": "w78lBV7UwCUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))\n",
        "input = torch.randn(3 * 2 * 2, 12)\n",
        "output = fold(input)\n",
        "output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "antzYlKSyl2M",
        "outputId": "ad35b4d1-4f8d-4f51-b306-b9894fa35ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_image(patches, image_size, patch_size = 7, patch_stride = 2):\n",
        "  b, n, e = patches.shape\n",
        "  res = patches.view(b, e, n)\n",
        "  counts = torch.ones_like(torch.empty(e, n))\n",
        "  fold = nn.Fold(output_size=(image_size, image_size), kernel_size=(patch_size, patch_size), stride = patch_stride)\n",
        "  counts = fold(counts)\n",
        "  res = fold(res)\n",
        "  return torch.div(res, counts)\n",
        "\n",
        "image_batch = torch.randn(2, 3, 128, 128)\n",
        "img_size = 128\n",
        "patch_size = 5\n",
        "\n",
        "stride = 2\n",
        "\n",
        "extracted_patches = extract_patches(image_batch, patch_size, stride)\n",
        "print(extracted_patches.shape)\n",
        "rec_img = reconstruct_image(extracted_patches, img_size, patch_size, stride)\n",
        "print(rec_img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veumWQJ1wjJz",
        "outputId": "b6e6ce7b-239b-4f84-ca2d-ad6a512d41f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3844, 75])\n",
            "torch.Size([2, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct the network"
      ],
      "metadata": {
        "id": "QEp1dcdg1Mm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_block1 = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        ")\n",
        "decoder_block2 = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='bilinear')\n",
        ")\n",
        "decoder_block3 = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, k, patch_size, stride, conv_type):\n",
        "        super(MyNet, self).__init__()\n",
        "        model = models.vgg19(pretrained=True)\n",
        "\n",
        "        self.enc_1 = model[:4]  # input -> relu1_1\n",
        "        self.enc_2 = model[4:11]  # relu1_1 -> relu2_1\n",
        "        self.enc_3 = model[11:18]  # relu2_1 -> relu3_1\n",
        "        self.enc_4 = model[18:27]  # relu3_1 -> relu4_1\n",
        "        self.dec_1 = decoder_block1\n",
        "        self.dec_2 = decoder_block2\n",
        "        self.dec_3 = decoder_block3\n",
        "        self.graph = GraphBlock(n_c=512, conv=conv_type, k=k, patch_size=patch_size, stride=stride)\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        self.update_layers = nn.Sequential(self.graph, self.dec_2, self.dec_3)\n",
        "\n",
        "        # fix the encoder\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1  from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        for i in range(4):\n",
        "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
        "        return input\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        #assert (input.size() == target.size())\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        #assert (input.size() == target.size())\n",
        "        #assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def forward(self, content, style, alpha=1.0):\n",
        "        # encoder\n",
        "        assert 0 <= alpha <= 1\n",
        "        style_feats = self.encode_with_intermediate(style)\n",
        "        content_feat = self.encode(content)\n",
        "        style_feat = self.encode(style)\n",
        "        # decoder\n",
        "        t = content_feat\n",
        "        content_t = self.graph(style_feat, content_feat)\n",
        "        content_t = adaptive_instance_normalization(content_t, style_feat)\n",
        "\n",
        "        content_t = self.dec_2(content_t)\n",
        "        output = self.dec_3(content_t)\n",
        "        g_t_feats = self.encode_with_intermediate(output)\n",
        "\n",
        "        # loss\n",
        "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
        "        loss_s = 0.\n",
        "        for i in range(3):\n",
        "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
        "        return output, loss_c, loss_s"
      ],
      "metadata": {
        "id": "YNWLDm9Z1Lr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}